{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270c690a-1163-437b-9e62-46a843b68ee1",
   "metadata": {},
   "source": [
    "# Assignment No. 18 (Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112b00a-c079-430a-b859-9be98ee8bec7",
   "metadata": {},
   "source": [
    "**Yogesh Sabale, Roll No. : 130**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a858c79-c2c9-4606-be5e-ee171421a955",
   "metadata": {},
   "source": [
    "## Solutions >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d3d1ef-ec06-4d97-851e-9defda09daf7",
   "metadata": {},
   "source": [
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdcf35",
   "metadata": {},
   "source": [
    "## 1. What is a Naïve Bayes Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a2a6c",
   "metadata": {},
   "source": [
    "* The Naïve Bayes classifier is a popular supervised machine learning algorithm used for classification tasks such as text classification.\n",
    "* Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem, with an assumption of strong independence between the features.\n",
    "* It is used for high dimensions data.\n",
    "* Naïve Bayes classifiers are commonly used in text classification, spam filtering, sentiment analysis, and recommendation systems.\n",
    "* Naïve Bayes classifiers are simple and powerful\n",
    "\n",
    "\n",
    "Some key characteristics and components of a Naïve Bayes classifier include:\n",
    "\n",
    "1. **Bayes' Theorem:**\n",
    "   - The fundamental concept behind Naïve Bayes is Bayes' theorem, which is a mathematical formula that calculates the probability of an event based on the probabilities of certain related events. In the context of Naïve Bayes, it helps compute the probability of a class given the features of an instance.\n",
    "\n",
    "2. **Conditional Independence Assumption:**\n",
    "   - The Naïve Bayes classifier assumes that the features used to describe instances are conditionally independent given the class label. Although this assumption may not hold true in all cases, the model often performs surprisingly well in practice.\n",
    "\n",
    "3. **Types of Naïve Bayes Classifiers:**\n",
    "   - There are different variants of Naïve Bayes classifiers, including:\n",
    "     - **Gaussian Naïve Bayes:** Assumes that continuous features follow a Gaussian (normal) distribution.\n",
    "     - **Multinomial Naïve Bayes:** Suitable for discrete data, often used in text classification with term frequency features.\n",
    "     - **Bernoulli Naïve Bayes:** Appropriate for binary feature data, commonly used in document classification tasks.\n",
    "\n",
    "4. **Text Classification:**\n",
    "   - Naïve Bayes is widely employed in text classification tasks, such as spam detection and sentiment analysis, where the features are often word frequencies or presence/absence of specific words.\n",
    "\n",
    "5. **Efficiency and Scalability:**\n",
    "   - Naïve Bayes classifiers are computationally efficient and scale well with high-dimensional data, making them suitable for large datasets.\n",
    "\n",
    "6. **Limited Data Requirements:**\n",
    "   - Naïve Bayes can perform reasonably well even with limited training data, which is an advantage in situations where obtaining large labeled datasets is challenging.\n",
    "\n",
    "7. **Interpretability:**\n",
    "   - The model's predictions are interpretable, as it provides probabilities for each class, indicating the likelihood of an instance belonging to a particular class.\n",
    "\n",
    "While Naïve Bayes has its strengths, it may not perform well in cases where the independence assumption is violated or when intricate dependencies among features significantly affect the classification task. Despite its simplicity, Naïve Bayes classifiers are known for their effectiveness in certain applications, especially in text and document classification scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58254a6",
   "metadata": {},
   "source": [
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9cf35c",
   "metadata": {},
   "source": [
    "## 2. What are the different types of Naive Bayes classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad810f8e",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers come in several variants, each suited for different types of data and applications. The three main types of Naive Bayes classifiers are:\n",
    "\n",
    "Each of these Naive Bayes variants assumes the same basic principles of the Naive Bayes algorithm, including the conditional independence assumption and the use of Bayes' theorem to calculate class probabilities. The choice of the specific variant depends on the nature of the data and the characteristics of the features. Here's a brief summary of each:\n",
    "\n",
    "1. **Gaussian Naive Bayes:**\n",
    "      - **Use Case:** Continuous features assumed to follow a Gaussian distribution.\n",
    "      - **Example Applications:** Numeric data such as measurements, sensor readings.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "      - **Use Case:** Discrete data with counts or frequencies (commonly used in text classification).\n",
    "      - **Example Applications:** Document classification, spam filtering, topic classification.\n",
    "\n",
    "3. **Bernoulli Naive Bayes:**\n",
    "      - **Use Case:** Binary data representing the presence or absence of features.\n",
    "      - **Example Applications:** Document classification, sentiment analysis.\n",
    "\n",
    "The choice of which type to use depends on the characteristics of your dataset and the assumptions that align with the nature of your features. It's common to try multiple variants and assess their performance to determine the most suitable one for a particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c7637",
   "metadata": {},
   "source": [
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa1218",
   "metadata": {},
   "source": [
    "## 3. Why Naive Bayes is called Naive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585542e",
   "metadata": {},
   "source": [
    "*Naive Bayes classifier assumes features are independent of each other. Since that is rarely possible in real-life data, the classifier is called naive.*\n",
    "\n",
    "The term \"naive\" in \"Naive Bayes\" refers to a simplifying assumption made by the algorithm regarding the independence of features. The Naive Bayes classifier assumes that the features used to describe instances are conditionally independent given the class label. In other words, the presence or absence of a particular feature does not affect the presence or absence of another feature, given the class.\n",
    "\n",
    "This assumption is considered \"naive\" because, in reality, features in a dataset are often correlated, and their dependencies are not fully captured by assuming independence. In more formal terms, the Naive Bayes classifier assumes that all features contribute independently to the probability of an instance belonging to a particular class, which is a simplification made for computational tractability.\n",
    "\n",
    "Despite its simplicity and the \"naive\" assumption, the Naive Bayes classifier has been found to work surprisingly well in many real-world applications, especially in text classification tasks. It is computationally efficient, easy to implement, and often performs competitively with more complex models, particularly in situations where the independence assumption holds reasonably well or where the lack of complete independence does not significantly impact classification accuracy.\n",
    "\n",
    "The term \"naive\" is used to highlight the simplicity of the assumption rather than to criticize the algorithm. While the independence assumption may not always be true in practice, Naive Bayes can still be effective in various applications, making it a popular choice for certain types of classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23f766",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b429b",
   "metadata": {},
   "source": [
    "## 4. Can you choose a classifier based on the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03819b4",
   "metadata": {},
   "source": [
    "The choice of a Naive Bayes classifier variant (Gaussian, Multinomial, or Bernoulli) is typically more influenced by the nature of the data and the characteristics of the features rather than the size of the training set. However, the size of the training set can still play a role in certain considerations:\n",
    "\n",
    "1. **Size of the Training Set:**\n",
    "   - **Large Training Set:** With a large training set, you may have more data to estimate the parameters of the probability distributions used in the model. This can be beneficial for Gaussian Naive Bayes and may lead to more accurate parameter estimates, especially when dealing with continuous features.\n",
    "\n",
    "   - **Small Training Set:** In situations where you have a small training set, Naive Bayes classifiers, in general, can still be effective due to their simplicity and the ability to handle limited data. However, a small training set may result in less accurate parameter estimates, and the classifier might be more prone to overfitting.\n",
    "\n",
    "2. **Nature of the Data:**\n",
    "   - **Continuous Features:** If your features are continuous, Gaussian Naive Bayes may be more appropriate, especially when dealing with a large training set. It assumes that features follow a Gaussian distribution.\n",
    "\n",
    "   - **Discrete Features:** For discrete features, you can choose between Multinomial and Bernoulli Naive Bayes based on the nature of your data. Multinomial Naive Bayes is suitable for features representing counts or frequencies, often used in text classification. Bernoulli Naive Bayes is appropriate for binary features.\n",
    "\n",
    "3. **Application Requirements:**\n",
    "   - Consider the requirements of your specific application. For example, if you are working with text data, Multinomial Naive Bayes is commonly used. If you are dealing with binary data, such as presence/absence features, Bernoulli Naive Bayes might be more appropriate.\n",
    "\n",
    "4. **Cross-Validation:**\n",
    "   - Regardless of the size of the training set, it's advisable to use techniques like cross-validation to assess the performance of different Naive Bayes variants on your specific dataset. Cross-validation helps provide a more robust estimate of the model's performance and helps in model selection.\n",
    "\n",
    "In summary, while the size of the training set is an important consideration, the nature of the data, the characteristics of the features, and the requirements of the specific application should guide the choice of the Naive Bayes variant. It's often beneficial to experiment with different variants and evaluate their performance using cross-validation to make an informed decision based on the characteristics of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29f17e",
   "metadata": {},
   "source": [
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6b332",
   "metadata": {},
   "source": [
    "## 5. Explain Bayes Theorem in detail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14b04b",
   "metadata": {},
   "source": [
    "Bayes' Theorem is a fundamental concept in probability theory that provides a way to update probabilities based on new evidence. It is named after the Reverend Thomas Bayes, an 18th-century statistician and theologian who introduced the theorem. Bayes' Theorem is widely used in various fields, including statistics, machine learning, and artificial intelligence.\n",
    "\n",
    "The theorem is expressed mathematically as follows:\n",
    "\n",
    "$$  P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}  $$\n",
    "\n",
    "Here's a detailed explanation of each term in Bayes' Theorem:\n",
    "\n",
    "- **P(A | B)**: This is the **posterior probability** of event A given that event B has occurred. It represents the updated probability of A after taking into account the information provided by B.\n",
    "\n",
    "- **P(B | A)**: This is the **likelihood or conditional probability** of event B given that event A has occurred. It represents the probability of observing B if A is true.\n",
    "\n",
    "- **P(A)**: This is the **prior probability** of event A, representing the initial probability of A before considering any new evidence.\n",
    "\n",
    "- **P(B)**: This is the **marginal probability** of event B, representing the overall probability of observing B, irrespective of the occurrence of A.\n",
    "\n",
    "The intuition behind Bayes' Theorem can be explained through the following steps:\n",
    "\n",
    "1. **Prior Probability (\\( P(A) \\)):**\n",
    "   - Start with an initial belief or probability of the event A before observing any new evidence.\n",
    "\n",
    "2. **Likelihood (\\( P(B | A) \\)):**\n",
    "   - Consider the probability of observing the new evidence B given that the event A is true.\n",
    "\n",
    "3. **Evidence (\\( P(B) \\)):**\n",
    "   - Consider the overall probability of observing the evidence B, regardless of whether A is true or false.\n",
    "\n",
    "4. **Posterior Probability (\\( P(A | B) \\)):**\n",
    "   - Calculate the updated probability of A given the new evidence B. This is the probability you are interested in after taking into account the new information.\n",
    "\n",
    "The strength of Bayes' Theorem lies in its ability to update probabilities as new evidence becomes available. It is a cornerstone in Bayesian statistics, where probabilities are treated as measures of belief or uncertainty. In machine learning, Bayes' Theorem is a key component of algorithms like Naive Bayes classifiers and Bayesian networks, where it is used for probabilistic reasoning and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29567dc8",
   "metadata": {},
   "source": [
    "*************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18992dde",
   "metadata": {},
   "source": [
    "## 6. What is the formula given by the Bayes theorem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18751be6",
   "metadata": {},
   "source": [
    "Bayes' Theorem is expressed mathematically as:\n",
    "\n",
    "$$  P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}  $$\n",
    "\n",
    "In this formula:\n",
    "\n",
    "- \\( P(A | B) \\) is the posterior probability of event A given that event B has occurred. It represents the updated probability of A after taking into account the information provided by B.\n",
    "\n",
    "- \\( P(B | A) \\) is the likelihood or conditional probability of event B given that event A has occurred. It represents the probability of observing B if A is true.\n",
    "\n",
    "- \\( P(A) \\) is the prior probability of event A, representing the initial probability of A before considering any new evidence.\n",
    "\n",
    "- \\( P(B) \\) is the marginal probability of event B, representing the overall probability of observing B, irrespective of the occurrence of A.\n",
    "\n",
    "The formula allows you to update your belief in the probability of event A based on new evidence B. It is widely used in Bayesian statistics and has applications in various fields, including machine learning and artificial intelligence."
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ee1b386b",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627002f",
   "metadata": {},
   "source": [
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d7ada6",
   "metadata": {},
   "source": [
    "## 7. What is posterior probability and prior probability in Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5290ba4d",
   "metadata": {},
   "source": [
    "In the context of Naive Bayes, the terms \"posterior probability\" and \"prior probability\" refer to specific probabilities used in Bayes' Theorem to make predictions. Let's break down these concepts:\n",
    "\n",
    "1. **Prior Probability (P(Class) or P(A)):**\n",
    "   - The prior probability represents the initial probability or belief in the occurrence of a particular class before considering any evidence. In the context of Naive Bayes, it is the probability of a class without taking into account the observed features. The prior probability is denoted as \\( P(\\text{Class}) \\).\n",
    "\n",
    "2. **Posterior Probability (P(Class | Feature) or P(A|B)):**\n",
    "   - The posterior probability represents the probability of a class given the observed features. It is updated based on the prior probability and the likelihood of observing the features given the class. In the context of Naive Bayes, the posterior probability is denoted as \\( P(\\text{Class} | \\text{Features}) \\).\n",
    "\n",
    "\n",
    "\n",
    "In the context of Naive Bayes classification, the prior probability represents the initial belief in the likelihood of each class, and the posterior probability is the updated probability based on the observed features.\n",
    "\n",
    "When making predictions, the Naive Bayes algorithm calculates the posterior probability for each class given the features and assigns the class with the highest posterior probability as the predicted class. The prior probability serves as an important factor in this calculation, influencing the starting point of the prediction process before considering the evidence provided by the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e7366",
   "metadata": {},
   "source": [
    "************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db04b7",
   "metadata": {},
   "source": [
    "## 8. Define likelihood and evidence in Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550a842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c4c76e",
   "metadata": {},
   "source": [
    "**********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bee219",
   "metadata": {},
   "source": [
    "## 10. How does the Naive Bayes classifier work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802c44ae",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem. It is particularly useful for classification tasks and is known for its simplicity and efficiency. Here's an overview of how the Naive Bayes classifier works:\n",
    "\n",
    "1. **Bayes' Theorem:**\n",
    "   - The foundation of the Naive Bayes classifier is Bayes' theorem, which relates the conditional and marginal probabilities of random events. For a classification problem, it is expressed as:\n",
    "\n",
    "    $$  P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}  $$\n",
    "\n",
    "   - In this formula:\n",
    "      - \\( P(A | B) \\) is the posterior probability of class C given the features X.\n",
    "      - \\( P(B | A) \\) is the likelihood, representing the probability of observing features X given class C.\n",
    "      - \\( P(A) \\) is the prior probability of class C.\n",
    "      - \\( P(B) \\) is the evidence, representing the overall probability of observing features X.\n",
    "\n",
    "2. **Naive Independence Assumption:**\n",
    "   - The \"Naive\" in Naive Bayes comes from the assumption of feature independence given the class. It assumes that the presence (or absence) of a particular feature is independent of the presence (or absence) of any other feature, given the class label. Although this assumption is often unrealistic in real-world data, the Naive Bayes classifier can still perform surprisingly well in practice.\n",
    "\n",
    "\n",
    "3. **Training:**\n",
    "   - The Naive Bayes classifier is trained using a labeled dataset. It estimates the prior probabilities \\( P(C) \\) for each class and the likelihoods \\( P(x_i | C) \\) for each feature given each class.\n",
    "\n",
    "4. **Prediction:**\n",
    "   - When making predictions on new, unseen data, the algorithm calculates the posterior probabilities for each class using Bayes' theorem.\n",
    "   - The class with the highest posterior probability is assigned as the predicted class for the given input.\n",
    "\n",
    "5. **Example:**\n",
    "   - Suppose you have a dataset of emails labeled as spam or non-spam, and the features are the presence or absence of certain words. The Naive Bayes classifier would estimate the likelihood of observing each word given the class (spam or non-spam) during training. When a new email comes in, it calculates the posterior probabilities for both classes based on the presence or absence of words, and classifies the email as spam or non-spam.\n",
    "\n",
    "6. **Types of Naive Bayes:**\n",
    "   - There are different variants of Naive Bayes, such as:\n",
    "      - **Gaussian Naive Bayes:** Assumes that the features follow a Gaussian distribution.\n",
    "      - **Multinomial Naive Bayes:** Suitable for discrete data, often used for text classification.\n",
    "      - **Bernoulli Naive Bayes:** Suitable for binary data, similar to Multinomial Naive Bayes but used when features are binary variables.\n",
    "\n",
    "7. **Advantages:**\n",
    "   - Naive Bayes is computationally efficient and easy to implement.\n",
    "   - It often performs well, especially on text classification tasks.\n",
    "\n",
    "8. **Limitations:**\n",
    "   - The assumption of feature independence may not hold in all real-world scenarios.\n",
    "   - It can be sensitive to irrelevant features.\n",
    "   - The probability estimates can be influenced by the distribution of the training data.\n",
    "\n",
    "Despite its simplicity and naive assumptions, Naive Bayes is a powerful and widely used algorithm, especially in situations where computational resources are limited or when dealing with high-dimensional data like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c36be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
